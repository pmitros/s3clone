#!/usr/bin/env python
'''
This is a script which allows you to **quickly** download large
numbers of files from Amazon S3. It does this by opening a large
number of connections, and downloading in parallel.

It is not the best thing for small numbers of large files. There
are better tools for that.
'''

import Queue
import argparse
import gzip
import os
import sys
import threading
import time
import shutil
import boto3
import itertools

from s3clone import humanize

from s3clone import helpers


DEBUG = False

## Parse arguments and initialize connection to Boto
args = helpers.parser.parse_args()

print "gzip", args.gzip
print "role", args.role
print "Workers", args.workers

if args.role:
    session = boto3.Session(profile_name=args.role)
else:
    session = boto3.Session()
client = session.client('s3')

## Grab a list of all objects...
object_list = []
directories = set()

## Figure out directories, so we don't try to download them
print "Queuing bucket...",
for itemname, size in helpers.list_objects(client, args.bucket, args.prefix):  #, 50):itertools.islice(helpers.list_objects(client, args.bucket, args.prefix), 50):
    #print itemname, size
    print ".",
    object_list.append([itemname, size])
    directories.add(os.path.dirname(itemname))
print

## Enqueue objects 
task_queue = Queue.PriorityQueue()
lock = threading.Lock()

total_bytes = 0
count = 0

for itemname, item_bytes in object_list:
    # Omit directories
    if itemname in directories or itemname.endswith('/'):
        continue

    # Skip over files which exist
    filename = os.path.join(args.outputdir, itemname)
    if os.path.exists(filename):
        if os.stat(filename).st_size != item_bytes:
            print itemname, ": Size mismatch",
            print item_bytes, os.stat(filename).st_size
        continue
    if os.path.exists(filename+'.s3clonegz.gz'):
        continue

    # And add them to the queue....
    task_queue.put((itemname, filename, size), -item_bytes)
    count = count + 1
    total_bytes = total_bytes + item_bytes


## Start up the workers

bytes_remaining = total_bytes

def worker():
    '''
    Pull items out of the queue, and download them, until the queue is
    empty
    '''
    while True:
        (queue_item, filename, item_bytes) = task_queue.get()
        if DEBUG:
            print "Grabbing", queue_item
        try:
            helpers.download(client, args.bucket, queue_item, filename, args.gzip)
            with lock:
                global bytes_remaining
                bytes_remaining = bytes_remaining - item_bytes
        except:
            print "Download of ", queue_item, "failed"
            print sys.exc_info()
        task_queue.task_done()


if total_bytes == 0:
    print "Empty bucket"
    sys.exit(-1)

print "Running workers... (%s)".format(humanize.bytes(total_bytes))

for i in range(args.workers):
    t = threading.Thread(target=worker)
    t.daemon = True
    t.start()

print "Workers started..."

start_time = time.time()

while not task_queue.empty():
    time.sleep(0.1)
    print "Downloading. ", task_queue.unfinished_tasks,
    print " files remaining, (", count, ")",
    print humanize.bytes(bytes_remaining), "(", humanize.bytes(total_bytes),
    print ")", int(time.time()-start_time), "secs \r",

task_queue.join()

print "Done!"
